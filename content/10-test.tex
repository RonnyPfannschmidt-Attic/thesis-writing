\chapter{Evaluation}
\label{cha:eval}
Dieses Kapitel bewertet das Resultat der Implementation und
stellt fest in welchem Umfang die Ziele umgesetzt wurden.

\section{Umgesetzte Funktionale Kriterien}
\label{sec:eval:features}
Wie in \cref{tab:features} aufgelistet,
wurden die Kernfunktionen umgesetzt.

\input{content/featurelist}    

Außerdem wurden einige erweiterte Funktionen sowie eine der beispielhaften Erweiterungen umgesetzt.
Damit ist der Umfang des Prototypen ausreichend um die Eigenschaften des Systems zu bewerten und zu bestimmen ob das gewünschte Ziel erreichbar ist.


\section{Testumgebung}
\label{sec:eval:Testumgebung}

\subsection{Manuelle Tests}
Um das gesamte System zu testen, wird der physische Überblick (siehe \cref{fig:grob-layout-komponenten}) als Plan hergenommen.
Auf 3 Rechnern (für jeden Knoten der Datenbank einen) ist dieser umgesetzt worden.
Dabei wurden auf jedem Rechner für jede physische CPU ein Arbeiter gestartet .
Auf den Testsystemen waren je vier physische CPU's verfügbar.
Zusätzlich wurde willkürlich auf einem der Rechner der Manager gestartet.

Um Daten an das Testsystem zu bringen, wurde dabei das \verb|put.py| Tool mit beispielhaften Daten verwendet.
Zum Vorbereiten der Datenbank wurde jeweils ``couchdb-compose'' \cite{couchdb:compose} verwendet.
Anschließend wurde das System beobachtet.

\subsection{Automatische Tests}
Die automatischen Tests wurden unter Zuhilfenahme von pytest \cite{pytest:website} und seiner Erweiterung  pytest-couchdb \cite{pytest:couchdbkit} auf dem Entwicklungsrechner  ausgeführt.
Dazu wurde lediglich eine einzelne Datenbank verwendet.
Die automatischen Tests bereiten die Datenbank dabei vor jeder Ausführung der Tests selbstständig vor.

\section{Testfälle}
\label{sec:eval:testcases}

Diese Sektion beschreibt die einzelnen komplexen Tests des Systems.
Alle automatisch durchgeführten Tests sollen dabei fehlerfrei durchlaufen.

\subsection{Unittests}

Die Auflistung und Eigenschaften der UnitTests
ist für die gewählte Betrachtung nicht relevant und
daher wird darauf verzichtet.

\subsection{Funktionale Tests}
Die funktionalen Tests stellen die Funktionsweise einzelner Komponenten sicher.
Komponenten die im Prototypen nur eine einzige Aufgabe haben,
erhielten daher auch nur einzelne Tests, während andere Komponenten mehrere Tests erhielten.

Die Tests verteilen sich dabei wie folgt:

\begin{itemize}
    \item
        \textbf{Eingang}
        \begin{itemize}
            \item Einfache Validation
            \item Arbeitspakete ohne eigene Achsen generieren
            \item Arbeitspakete mit eigenen Achsen generieren
        \end{itemize}
    \item
        \textbf{Management}
        \begin{itemize}
            \item Arbeitsschritte vom Template generieren
            \item Arbeitspaket Zuteilung konfliktfrei
            \item Arbeitspaket Zuteilung konfliktbehaftet
        \end{itemize}
    \item
        \textbf{Arbeiter}
        \begin{itemize}
            \item Arbeitspaket anfordern konfliktfrei
            \item Arbeitspaket anfordern konfliktbehaftet
            \item Zugewiesenes Arbeitspaket ausführen
        \end{itemize}
    \item
        \textbf{Arbeitsschritte}
        \begin{itemize}
            \item
                \textbf{Prozess}
                \begin{itemize}
                    \item Einfacher Prozess
                    \item Einfacher Prozess Fehlschlag
                    \item Prozess mit vielen ausgaben (cat)
                    \item Python Prozess
                \end{itemize}
            \item
                \textbf{SCM}
                \begin{itemize}
                    \item Clone lokal
                    \item Clone remote
                    \item Clone Fehlschlag
                \end{itemize}
        \end{itemize}
\end{itemize}



\subsection{Systemtests}

Von den drei Systemtests wurden nur zwei automatisiert.
Die Tests mit dem Durchlauf der Funktionalen Komponenten und des Gesamtsystems
zeigten viele kleine Fehler, auf die hier nicht näher eingegangen wird.

Das Testen der Beispielerweiterung stellte sich als kompliziert heraus.
Die notwendige Systemumgebung zur schaffen war schwer zu automatisieren und daher wurde sie manuell getestet.

Der Durchlauf der Komponenten führt einfach nur die Komponenten der logischen Reihenfolge nach aus:
\begin{enumerate}
    \item Auftrag überprüfen,
    \item Auftrag vorbereiten,
    \item Auftrag Arbeitspakete generieren,
    \item Arbeitspaket Schritte generieren,
    \item Arbeitspaket in Anspruch nehmen,
    \item Arbeitspaket zuweisen,
    \item Arbeitspaket erwarten und
    \item Arbeitspaket ausführen.
\end{enumerate}

Der Test des Gesamtsystems verwendet die beschriebenen Systemkomponenten um den Manager und 2 Arbeiter zu starten.
Anschließend gibt er Aufträge verschiedener Größe ins System.

\subsection{Manuelle Tests}

Alle manuellen Tests wurden im Kontext der ``Großen Datenanalyse'' durchgeführt.
Der Umfang dieses Tests ermöglichte es wiederholt Fehlerfälle zu testen sowie das Systemverhalten zu beobachten.

Zu verschiedenen Zeitpunkten wurden Komponenten des Systems hart beendet und später wieder neu gestartet.
Dabei wurde das Verhalten der Komponenten nach dem Neustart
% im vergleich zu regul. Ausfuehrung
 sowie der Einfluss des Verlustes auf das Gesamtsystem betrachtet.


\section{Festgestellte Probleme}
\label{sec:eval:probleme}

Dieses Unterkapitel  beschreibt die in der Implementation und Testphase aufgetretenen Probleme.

\subsection{Manuelle Tests}

Bei den manuellen Tests sind einige schwerwiegende Probleme aufgetreten.
Diese werden hier nur kurz umrandet.

\begin{itemize}
    \dhitem[Langwierige Anlaufphase:]
        Beim wiederholten Testen mit der gleichen Datenbank fiel auf,
        dass die Menge der Daten in der Datenbank Einfluss auf das Starten von Komponenten hat.
        Je mehr Daten sich in der Datenbank befinden, desto länger benötigen Komponenten,
        um mit ihrer Arbeit wieder zu beginnen.
    \dhitem[Konfliktsituation bei der Inanspruchname von Arbeitspacketen:]
        Beim Testen mit vielen Arbeitern fiel auf, dass immer alle gerade freien Arbeiter
        dasselbe nächste Arbeitspaket in Anspruch nehmen wollten. Dies führte oft dazu,
        dass manche Arbeiter erst nach dutzenden Versuchen das nächste Arbeitspaket bearbeiten konnten.
    \dhitem[Blockade bei der Abarbeitung von Managementdaten:]
        Beim Absetzen großer Aufträge fiel auf, dass der Manager in einigen Situationen
        für mehrere Minuten keine Inanspruchnahme eines Arbeitspaketes bearbeitete.
        Somit wurden Arbeitern keine Arbeitsschritte gegeben, obwohl diese eigentlich verfügbar waren.
    \dhitem[Blockade der Datenausgabe von Python:]
        Bei der Beobachtung des Systems fiel auf, dass Python Prozesse
        die Daten von STDOUT/STDERR nicht kontinuierlich und zeilenweise,
        sondern in größeren Blöcken zur Ausgabe schickten.
\end{itemize}


\subsection{Datenbank und Erweiterungen}

Bei der Implementation der Erweiterung für Datenanalyse fiel auf,
dass die Resultate des Map-Reduce-Algorithmus an die verfügbaren Informationen an der Datenbank gebunden sind.
Es ist nicht möglich für Daten aus Events einen $Key$ mit Informationen aus den zugeordneten Arbeitspaketen zu generieren ohne zuvor eine starke Denormalisierung einzuführen.

Damit ist die Sortierung der Resultate an die verwendeten Schlüssel gebunden. 
Im Prototypen wurde dieses Problem mittels nachgelagerter Sortierung gelöst,
allerdings ist eine Lösung in der Datenbank wünschenswert da die nach-gelagerte Sortierung einen erheblichen Mehraufwand bedeutete.


\section{Zusammenfassung}
\label{sec:eval:zusammenfassung}
%XXX
Um die drei definierten Systemkriterien als umgesetzt zu betrachten,
müssten die festgestellten Probleme gelöst werden,
dies soll soweit technisch möglich im folgenden Kapitel behandelt werden.

